---
title: "Clustering Companies Based on Stock Price Movements"
author: "ASLIHAN_DEMIRKAYA"
output: html_document
---

## Introduction

In this work, we will apply Principal Component Analysis (PCA), t-Distributed Stochastic Neighbor Embedding (TSNE), K-means clustering and Hierarchical clustering. We will construct our dataset using the two datasets that can be obtained here: https://www.kaggle.com/ehallmar/daily-historical-stock-prices-1970-2018. The two datasets that we use as our source, have daily stock prices for a selection of several thousand stock tickers from NYSE and NASDAQ. The dataset we will work on covers the dates between '2017-08-24' AND '2018-08-24' and the 11 industries with the most number of observations. Our aim is to cluster the records so that the companies in the same industry will be grouped together.

 

## Exploring the Dataset

As mentioned earlier, we will use two datasets as our source. Both are very large in size so we will use `data.table` to increase reading speed. The first dataset is `historical_stock_prices.csv` and the second one is `historical_stocks.csv`. The first and the second datsets have `ticker` in common. While constructing our data, we will need features from both sets. Just a note, we will use `SQL` for table manipulation.

The first dataset mostly has numerical variables as follows:

```{r}
library(data.table) #for big data
historical_stock_prices<-as.data.frame(fread("historical_stock_prices.csv")) #the table is changed to data frame
head(historical_stock_prices)
```


The second data set has categorical data as follows:

```{r}
historical_stocks<-as.data.frame(fread("historical_stocks.csv"))
head(historical_stocks)
```

As we mentioned earlier, we are going to use `SQL` in `r`. To do this, we need the library `sqldf`. We need to inner join these two datasets and will select the features we need. Note that both datasets have `ticker` in common.

```{r}
library(sqldf)
stocks<-sqldf("SELECT open, date,hsp.ticker, hs.industry
               FROM historical_stock_prices AS hsp
               INNER JOIN historical_stocks AS hs
               ON hsp.ticker=hs.ticker  
      ")
```

Now we are interested in the observations of `stocks` that are between '2017-08-24' and '2018-08-24'. In the below code, we construct the dataset `stocks_2018`.

```{r}
stocks_2018<-sqldf("SELECT *
                    FROM stocks
                    WHERE date BETWEEN '2017-08-24' AND '2018-08-24'
      "
)
```
```{r}
head(stocks_2018)
```

Now, we will pick the `industry` with the highest count in `ticker`. What we mean by this is for instance there are `n` companies that belong to the industry `A`. We will pick the first 11 industries where the corresponding `n` values of these industries are the first 11 when ordered from biggest to smallest.	

Let's start this process by selecting the two features `ticker` and `industry`. Note that we want the distinct pairs. Otherwise we will count the repeated pairs with distict dates. We do not want that. That's why we will do this in two steps. 

```{r}
df<-sqldf("SELECT DISTINCT ticker,industry
           FROM stocks_2018
      ")
```
```{r}
head(df)
```

The second step is to count the `ticker` for the corresponding industry and pick the first 12. The reason why we pick 12 but not 11 is the `N/A` values and as seen below it has the highest count.

```{r}
df2<-sqldf(" SELECT industry, COUNT(*)
             FROM df
             GROUP BY industry
             ORDER BY COUNT(*) DESC
             LIMIT 1,12   
 "      )
```


```{r}
head(df2)
```


After we determine our 11 industries, now we will construct the data set `stocks_11industry` that has all the obervations with the corresponding 11 industries. We do this by using the following `SQL` code:

```{r}
stocks_11industry<-sqldf("SELECT open, date, s2018.ticker, df2.industry
                          FROM stocks_2018 AS s2018
                          INNER JOIN df2
                          ON s2018.industry=df2.industry
      ")
```

```{r}
head(stocks_11industry)
```


### Changing the `date` format

As seen above, the type of the variable `date` is character. We will change its format to `Date`. The reason is that `date` variable is in its character format, and the `lubridate` package makes possible to process it as a date object.  

```{r}
library(lubridate)
stocks_11industry$date<-ymd(stocks_11industry$date) #changes the type from char to Date
str(stocks_11industry)
```

In the below `SQL` code, we create a new variable `no`. For each `ticker`, the dates will be ordered and each date for that corresponding `ticker` will be assigned to a number. The earliest date will be assigned to 1, the next business day will be assigned to 2, and so on. Note that if there were no none business days, such as weekends, we wouldn't need that additional variable.

```{r}
added_no<-sqldf("SELECT row_number() 
                 OVER(PARTITION BY ticker ORDER BY date) AS no, * 
                 FROM stocks_11industry
         ")
```
Now, our aim is to add a new variable `open2` to our dataframe. This variable is same as `open` however, it is one business day shifted. We will use this variable `open2` and `open` to calculate the daily return. We also change the name of the variable from `open` to `open1`.

```{r}
added_open2<-sqldf("SELECT ar1.open AS open1, ar1.date,ar1.ticker,ar1.industry,ar1.no,
                           ar2.no, ar2.open AS open2
                    FROM added_no AS ar1
                    INNER JOIN added_no AS ar2
                    ON ar1.ticker=ar2.ticker AND ar2.no=(ar1.no)+1
      ")
```

Now let's look at our new dataframe with the new variables. 

```{r}
head(added_open2)
```

Now we are going to add a new variable `daily_return` and call the new data frame as `data_daily_return`. The formula we use is as follows:

$$\text{daily_return}=\frac{open2}{open1}*100-100$$

```{r}
added_daily_return<-sqldf(
      "SELECT date,ticker, industry, no, open1, open2,(open2/open1)*100-100 AS daily_return
       FROM added_open2
      ")
```

Now let's look at our new data frame `data_daily_return`.
```{r}
head(added_daily_return)
```

### Dealing with Outliers

Before we apply our methods, we would like to deal with the outliers for the variable `daily_return`. We will not remove them, instead, we will replace these values with the upper or lower whisker values for the corresponing `ticker`. 

So let's group the data by the ticker and also the date.

```{r}
grouped_date_ticker<-sqldf(
      "SELECT date,ticker, industry, daily_return,no
       FROM added_daily_return
       GROUP BY ticker,date
      ")
```

```{r}
head(grouped_date_ticker)
```
Let's look at the summary statistics of `daily_return`.

```{r}
summary(grouped_date_ticker$daily_return)
```

Here is the boxplot of the summary.

```{r}
boxplot(grouped_date_ticker$daily_return)
```

```{r}
Q1<-quantile(grouped_date_ticker$daily_return,0.25)
Q3<-quantile(grouped_date_ticker$daily_return,0.75)
max_open1<-Q3+(Q3-Q1)*1.5  #upper whisker value
min_open1<-Q1-(Q3-Q1)*1.5 # lower whisker value

OutVals = boxplot(grouped_date_ticker$daily_return,plot=FALSE)$out
outlier_index<-which(grouped_date_ticker$daily_return %in% OutVals)
```

```{r}
grouped_date_ticker$daily_return[outlier_index]<-ifelse(grouped_date_ticker$daily_return[outlier_index]>max_open1,max_open1,min_open1)
```

Now let's look at the summary and the boxplot of `daily_return` after we worked with the outliers.

```{r}
summary(grouped_date_ticker$daily_return)
boxplot(grouped_date_ticker$daily_return)
```

### Changing the `date` values into features

The idea is to see the `daily_return` for each `ticker` daily. We will use `spread` function from the `tidyr` package. We have to remove the `rn` variable, otherwise we do not get what we want. You can try and see as an exercise.

```{r}
library(tidyr)
grouped_date_ticker$no=NULL
stocks_daily_return <- grouped_date_ticker %>% spread(date, daily_return,fill=0)
head(stocks_daily_return[,1:6] )
```

Now, we are ready to apply our methods. We will start with PCA and then TSNE. We will compare both methods. Afterwards, we will study clustering and see if we get consistent results with K-means and hierarchial.

### Principal Component Analysis (PCA)

In this section, we are going to apply PCA. However, we should be careful with our varibles. PCA handles numerical values, not categorical ones. Also one of the requirements is scaling the dataset. However, we only have one variable to work with, that is `daliy_return` so there is no need to scale.

In the below code, we define a new data frame with only numerical features.
```{r}
rem_cat<-stocks_daily_return[c(-1,-2)] #we remove the categorical variables
head(rem_cat[,1:6])
```

Now we are ready to apply PCA to the data set `rem_cat`. Since we have 252 date features, we do not want to show the output of PCA. Instead, we only show the head of the loadings on the first 6 principal components.

```{r}
pca.out<-prcomp(rem_cat, scale=FALSE) 
df_out <- as.data.frame(pca.out$x)
head(df_out[,1:6])
```

Now let's use `biplot` to plot the PCA results where the axes are PC1 and PC2.

```{r}
biplot(pca.out, scale=0, cex=1.2) #cex is for the font
```

As seen the plot is very crowded and hard to detect the points, directions, etc. So let's try something else. For that purpose, we will use the package: `ggplot2` and for extra nicer look, we will add the packages: `grid` and `gridExtra`.

```{r}
library(ggplot2)
library(grid)
library(gridExtra)
p<-ggplot(df_out,aes(x=PC1,y=PC2,color=stocks_daily_return$industry ))
p<-p+geom_point()
p
```

From the figure, we can say that PCA did a good job in separating oil and gas production from other industries, but not with the rest of the industries.

### t-Distributed Stochastic Neighbor Embedding (TSNE) 

```{r}
## calling the installed package
library(Rtsne)
rem_cat$label <- stocks_daily_return$industry
rem_cat$label<-as.factor(rem_cat$label)
tsne <- Rtsne(rem_cat[,-1], dims = 2, perplexity=30, verbose=TRUE, max_iter = 1500)
```

As we did for the PCA, let's see the head of the two vectors of TSNE.
```{r}
df_out2 <- as.data.frame(tsne$Y)
head(df_out2)
```

When we plot the data values where the axes are V1 and V2, we get the following figure.

```{r}
p<-ggplot(df_out2,aes(x=V1,y=V2,color=stocks_daily_return$industry ))
p<-p+geom_point()
p
```

Looking at the above figure, we can say that TSNE did a good job in clustering the industries: oil and gas production, major banks, real estate investment trusts, computer software. 

As a conclusion, if we compare the performance of PCA and TSNE, we can say that TSNE did a better job.


### K-Means Clustering

In this section, we are going to apply K-Means Clustering to our dataset `rem_cat`. Remember that we added the variable `label` when we did TSNE analysis. We are going to remove it since we only want numerical values again.


```{r}
rem_cat$label=NULL   #removes the variable `label`
```

Since our dataset has only has 11 industries, we want to pick the number of clusters as 11 and see if K-means does a good job in estimating that.

```{r}
set.seed(123)
km.out=kmeans(rem_cat,11,iter.max = 20,nstart=25) #25 random starts
df3_centers <- as.data.frame(km.out$centers)
head(df3_centers[,1:6])
df3_cluster <- as.data.frame(km.out$cluster)
head(df3_cluster)
```

Here is the number of data values in each cluster.

```{r}
table(km.out$cluster)
```


Now, let's visually see these clusters. The figure we will plot has the axes PC1 and PC2. We are going to use the package `ggfortify` for that. Note that we have to change the column names of centers, otherwise we get an error message: "Position must be between 0 and n".

```{r}
library(ggfortify)
colnames(km.out$centers) <- 1:252
autoplot(km.out, data=rem_cat)
```

Now, let's compare our K-means results and the industries we had in our dataset. We can use `table` to see how well they match.  

```{r}
library(knitr)
d <- table(stocks_daily_return$industry,km.out$cluster)
kable(d)
```

Looking at the table, we can tell that K-Means did a fair job in clustering. For instance, 155 out of 215 data points that belong to Real Estate Investment Trusts were put in one cluster. 211 out of 328 of Major Banks were put in one cluster. 

### Elbow Method
Even though we picked the number of clusters as 11, it is worth trying to see if there is any optimal value for the number of clusters. Below is the code that tries for the number of clusters that starts from 2 and goes up to 15 by increasing the number by 1 at each run.

```{r}
#Elbow Method for finding the optimal number of clusters
set.seed(123)
# Compute and plot wss for k = 2 to k = 15.
k.max <- 15
data <- rem_cat
wss <- sapply(1:k.max, 
              function(k){kmeans(data, k, nstart=25,iter.max = 20 )$tot.withinss})
wss
plot(1:k.max, wss,
     type="b", pch = 19, frame = FALSE, 
     xlab="Number of clusters K",
     ylab="Total within-clusters sum of squares")

```

The total within-cluster sum of square measures the compactness of the clustering and we want it to be as small as possible. For k=11 seems like a good choice for number of clusters so we are not going to try for another k value.






### Hierarchical Clustering

In this final section, we are going to apply hierarchical clustering. We are going to use `hclust` function and pick our method as "complete".


```{r}
hc.complete<-hclust(dist(rem_cat), method="complete") #largest distance between the data points in the clusters
```

We will again pick the number of clusters are equal to 11.

```{r}
cut_avg <- cutree(hc.complete, k = 11)
table(cut_avg)
```

The table shows that most of the data values are in cluster 1 and 2. Let's visualize this by using the package `dendextend`.


```{r}
library(dendextend)
avg_dend_obj <- as.dendrogram(hc.complete)
avg_col_dend <- color_branches(avg_dend_obj, k = 11)
plot(avg_col_dend)
```

Now, let's compare our hierarchial clustering results and the industries we had in our dataset. We can use `table` to see how well they match. 

```{r}
d <- table(stocks_daily_return$industry,cut_avg)
kable(d)
```


By looking at this table, we can conclude that hierarchial clustering did not perform well, since most of the data points were assigned to one cluster.



